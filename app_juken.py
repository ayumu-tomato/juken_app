import streamlit as st
import pandas as pd
import google.generativeai as genai
import datetime
import PIL.Image
import json
import re
import time

# ==========================================
# ğŸ” åˆæœŸè¨­å®š & ãƒ¢ãƒ‡ãƒ«è‡ªå‹•æ¤œå‡º
# ==========================================
st.set_page_config(page_title="æ–°æ½Ÿé«˜æ ¡ åˆæ ¼ãƒŠãƒ“", layout="wide")
st.title("ğŸ”ï¸ æ–°æ½Ÿé«˜æ ¡ åˆæ ¼ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ & å¾¹åº•å¾©ç¿’")

try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    api_key = ""

if not api_key:
    st.warning("âš ï¸ Secretsã«APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

genai.configure(api_key=api_key)

# ---------------------------------------------------------
# ğŸ¤– æœ€å¼·ã®ãƒ¢ãƒ‡ãƒ«è‡ªå‹•æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ (Ver. 3.0å¯¾å¿œ)
# ---------------------------------------------------------
def get_best_pro_model():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ä¸­ã‹ã‚‰ã€Gemini 3.0ã‚’å«ã‚€æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã§æ¢ã™"""
    try:
        # APIã‚­ãƒ¼ã§ä½¿ãˆã‚‹å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        all_models = [m.name.replace("models/", "") for m in genai.list_models()]
        
        # å„ªå…ˆé †ä½ãƒªã‚¹ãƒˆï¼ˆæ–°ã—ã„é †ã«ä¸¦ã¹ã¦ã„ã¾ã™ï¼‰
        # ã“ã“ã« gemini-3-pro ç³»ã‚’è¿½åŠ ã—ã¾ã—ãŸ
        priority_list = [
            "gemini-3-pro",           # æœ¬å‘½
            "gemini-3-pro-preview",   # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ
            "gemini-3.0-pro",         # è¡¨è¨˜æºã‚Œå¯¾å¿œ
            "gemini-2.5-pro",         # 2.5ç³»
            "gemini-2.0-pro-exp",     # 2.0ç³»å®Ÿé¨“ç‰ˆ
            "gemini-1.5-pro-002",     # 1.5ç³»æœ€æ–°
            "gemini-1.5-pro-latest",
            "gemini-1.5-pro",
            "gemini-pro"
        ]
        
        # å„ªå…ˆãƒªã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€å®Ÿéš›ã«ä½¿ãˆã‚‹ã‚‚ã®ã‚’æ¢ã™
        for model_name in priority_list:
            if model_name in all_models:
                return model_name
        
        # ãƒªã‚¹ãƒˆã«ãªãã¦ã‚‚ "pro" ãŒã¤ããƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ï¼ˆæ•°å­—ãŒå¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆï¼‰
        pro_models = [m for m in all_models if "pro" in m and "vision" not in m]
        if pro_models:
            # åå‰ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸€ç•ªæ–°ã—ãã†ãªã‚‚ã®ã‚’è¿”ã™ (ä¾‹: gemini-1.5-pro > gemini-1.0-pro)
            pro_models.sort(reverse=True)
            return pro_models[0]
                
        # ã©ã†ã—ã¦ã‚‚ProãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿Flash
        return "gemini-1.5-flash"
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ç­–ã¨ã—ã¦ 1.5 Pro ã‚’è¿”ã™
        return "gemini-1.5-pro"

# ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
MODEL_NAME = get_best_pro_model()

# ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
try:
    model_main = genai.GenerativeModel(MODEL_NAME)
    model_vision = genai.GenerativeModel(MODEL_NAME)
    st.sidebar.success(f"ğŸš€ AI Model: {MODEL_NAME} (Connected)")
except Exception as e:
    st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã€{MODEL_NAME}ã€ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ---------------------------------------------------------
# ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼‰
# ---------------------------------------------------------
if 'data_store' not in st.session_state: st.session_state['data_store'] = {} # ç”Ÿãƒ‡ãƒ¼ã‚¿
if 'clean_df' not in st.session_state: st.session_state['clean_df'] = pd.DataFrame() # æ•´ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
if 'category_map' not in st.session_state: st.session_state['category_map'] = {} # å˜å…ƒå¤‰æ›è¾æ›¸
if 'textbooks' not in st.session_state: st.session_state['textbooks'] = {}

# 12åˆ†é¡å®šç¾©
FIXED_CATEGORIES = {
    "å›½èª": ["æ¼¢å­—", "æ–‡æ³•", "è©•è«–", "å¤æ–‡", "ãã®ä»–"],
    "æ•°å­¦": ["æ•°ã¨å¼", "æ–¹ç¨‹å¼ãƒ»ä¸ç­‰å¼", "é–¢æ•°(æ¯”ä¾‹ãƒ»1æ¬¡)", "é–¢æ•°(2æ¬¡ãƒ»ãã®ä»–)", "å¹³é¢å›³å½¢", "ç©ºé–“å›³å½¢", "å›³å½¢ã®è¨¼æ˜", "ç¢ºç‡", "ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨", "æ•´æ•°ãƒ»è¦å‰‡æ€§", "ä½œå›³", "èåˆå•é¡Œãƒ»ãã®ä»–"],
    "è‹±èª": ["å˜èªãƒ»èªå½™", "æ–‡æ³•(æ™‚åˆ¶ãƒ»å‹•è©)", "æ–‡æ³•(æº–å‹•è©ãƒ»é–¢ä¿‚è©)", "æ–‡æ³•(ãã®ä»–)", "é•·æ–‡èª­è§£(ç‰©èª)", "é•·æ–‡èª­è§£(èª¬æ˜æ–‡)", "è‹±ä½œæ–‡", "ãƒªã‚¹ãƒ‹ãƒ³ã‚°", "ä¼šè©±æ–‡", "èªé †æ•´åº", "é©èªè£œå……", "ãã®ä»–"],
    "ç†ç§‘": ["ç‰©ç†(å…‰ãƒ»éŸ³ãƒ»åŠ›)", "ç‰©ç†(é›»æ°—ãƒ»ç£ç•Œ)", "ç‰©ç†(é‹å‹•ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼)", "åŒ–å­¦(ç‰©è³ªãƒ»æ°—ä½“)", "åŒ–å­¦(å¤‰åŒ–ãƒ»åŸå­)", "åŒ–å­¦(ã‚¤ã‚ªãƒ³ãƒ»é›»æ± )", "ç”Ÿç‰©(æ¤ç‰©)", "ç”Ÿç‰©(å‹•ç‰©ãƒ»äººä½“)", "ç”Ÿç‰©(éºä¼ãƒ»é€²åŒ–)", "åœ°å­¦(ç«å±±ãƒ»åœ°å±¤)", "åœ°å­¦(å¤©æ°—ãƒ»æ°—è±¡)", "åœ°å­¦(å¤©ä½“)"],
    "ç¤¾ä¼š": ["åœ°ç†(ä¸–ç•Œ)", "åœ°ç†(æ—¥æœ¬)", "åœ°ç†(è³‡æ–™èª­å–)", "æ­´å²(å¤ä»£ï½ä¸­ä¸–)", "æ­´å²(è¿‘ä¸–)", "æ­´å²(è¿‘ç¾ä»£)", "å…¬æ°‘(ç¾ä»£ç¤¾ä¼šãƒ»äººæ¨©)", "å…¬æ°‘(æ”¿æ²»)", "å…¬æ°‘(çµŒæ¸ˆ)", "å…¬æ°‘(å›½éš›)", "èåˆå•é¡Œ", "ãã®ä»–"]
}

# ---------------------------------------------------------
# ğŸ› ï¸ é–¢æ•°å®šç¾©ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰
# ---------------------------------------------------------
def ask_gemini_robust(prompt, image_list=None):
    """Proãƒ¢ãƒ‡ãƒ«ç”¨ã®é ‘ä¸ˆãªå‘¼ã³å‡ºã—é–¢æ•°ï¼ˆè‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if image_list:
                response = model_vision.generate_content([prompt] + image_list)
            else:
                response = model_main.generate_content(prompt)
            return response.text
        except Exception as e:
            if "429" in str(e) or "Quota" in str(e):
                st.toast(f"â³ æ··é›‘ä¸­...æ•°ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™ ({attempt+1}/3)")
                time.sleep((attempt + 1) * 3)
            else:
                return f"ã‚¨ãƒ©ãƒ¼: {e}"
    return "âŒ æ··é›‘ã®ãŸã‚å¿œç­”ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦è©¦ã—ã¦ãã ã•ã„ã€‚"

def parse_csv(file):
    """CSVèª­ã¿è¾¼ã¿ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    try:
        file.seek(0)
        try:
            df = pd.read_csv(file, header=None)
        except:
            file.seek(0)
            df = pd.read_csv(file, header=None, encoding='cp932')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¢ã™
        header_row_mask = df.apply(lambda r: r.astype(str).str.contains('å¤§å•|å†…å®¹').any(), axis=1)
        if len(df[header_row_mask]) > 0:
            idx = df[header_row_mask].index[0]
            target_row = df.iloc[idx]
            col_idx = 0
            for c in df.columns:
                val = str(target_row[c])
                if 'å¤§å•' in val or 'å†…å®¹' in val:
                    col_idx = c
                    break
            
            subset = df.iloc[idx:, col_idx:].reset_index(drop=True).T
            # åˆ—åã‚’æ–‡å­—åˆ—åŒ–ã—ã¦ã‚¨ãƒ©ãƒ¼é˜²æ­¢
            subset.columns = [str(val).strip() for val in subset.iloc[0]]
            subset = subset[1:]
            
            if 'å¤§å•' in subset.columns: subset = subset.dropna(subset=['å¤§å•'])
            subset['ç‚¹æ•°'] = pd.to_numeric(subset['ç‚¹æ•°'], errors='coerce').fillna(0)
            subset['é…ç‚¹'] = pd.to_numeric(subset['é…ç‚¹'], errors='coerce').fillna(0)
            subset['ãƒ•ã‚¡ã‚¤ãƒ«å'] = str(file.name)
            
            for sub in ['æ•°å­¦','è‹±èª','ç†ç§‘','ç¤¾ä¼š','å›½èª']:
                if sub in file.name:
                    subset['æ•™ç§‘'] = sub
                    break
            else:
                subset['æ•™ç§‘'] = 'ãã®ä»–'
            
            if 'ç‚¹æ•°' in subset.columns:
                return subset
    except:
        pass
    return None

def process_and_categorize():
    """ã€é‡è¦ã€‘ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€AIã§å˜å…ƒã‚’æ•´ç†ã—ã¦ä¿å­˜ã™ã‚‹ä¸€æ‹¬å‡¦ç†"""
    if not st.session_state['data_store']:
        return

    # 1. å…¨ãƒ‡ãƒ¼ã‚¿çµåˆ
    raw_df = pd.concat(st.session_state['data_store'].values(), ignore_index=True)
    
    # 2. æœªçŸ¥ã®å˜å…ƒã‚’æ¢ã™
    unique_pairs = raw_df[['æ•™ç§‘', 'å†…å®¹']].drop_duplicates()
    unknown_list = []
    for _, row in unique_pairs.iterrows():
        subj = row['æ•™ç§‘']
        topic = str(row['å†…å®¹']).strip()
        if (subj, topic) not in st.session_state['category_map']:
            unknown_list.append(f"{subj}: {topic}")
    
    # 3. AIã«åˆ†é¡ã•ã›ã‚‹ï¼ˆæœªçŸ¥ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
    if unknown_list:
        status_text = st.empty()
        status_text.info(f"ğŸ¤– AI({MODEL_NAME})ãŒ {len(unknown_list)} ä»¶ã®æ–°ã—ã„å˜å…ƒã‚’åˆ†æãƒ»æ•´ç†ã—ã¦ã„ã¾ã™...")
        
        categories_str = json.dumps(FIXED_CATEGORIES, ensure_ascii=False, indent=2)
        prompt = f"""
        å­¦ç¿’å¡¾ã®æ•™å‹™ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚
        å…¥åŠ›ã•ã‚ŒãŸã€Œæ•™ç§‘: å˜å…ƒåã€ã‚’ã€ä»¥ä¸‹ã®ã€å®šç¾©æ¸ˆã¿ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã€‘ã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã«åˆ†é¡ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        
        ã€å®šç¾©æ¸ˆã¿ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã€‘
        {categories_str}
        
        ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‘
        """ + "\n".join(unknown_list)
        
        response = ask_gemini_robust(prompt)
        
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                mapping = json.loads(json_match.group())
                for k, v in mapping.items():
                    if ':' in k:
                        s, t = k.split(':', 1)
                        st.session_state['category_map'][(s.strip(), t.strip())] = v.strip()
                status_text.success("âœ… æ•´ç†å®Œäº†ï¼")
                time.sleep(1)
                status_text.empty()
        except:
            status_text.warning("âš ï¸ ä¸€éƒ¨ã®å˜å…ƒæ•´ç†ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")

    # 4. ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ä¿å­˜
    df_clean = raw_df.copy()
    if 'è©³ç´°' not in df_clean.columns: df_clean['è©³ç´°'] = df_clean['å†…å®¹']
    
    def apply_mapping(row):
        key = (row['æ•™ç§‘'], str(row['å†…å®¹']).strip())
        return st.session_state['category_map'].get(key, row['å†…å®¹']) # ãƒãƒƒãƒ—ã«ãªã‘ã‚Œã°ãã®ã¾ã¾

    df_clean['å†…å®¹'] = df_clean.apply(apply_mapping, axis=1)
    
    # 5. çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜ï¼ˆã“ã‚Œã§å†è¨ˆç®—ä¸è¦ï¼ï¼‰
    st.session_state['clean_df'] = df_clean

# ---------------------------------------------------------
# ğŸ–¥ï¸ ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
# ---------------------------------------------------------
with st.sidebar.form("textbook_form"):
    st.subheader("ğŸ“š å‚è€ƒæ›¸è¨­å®š")
    tb_math = st.text_input("æ•°å­¦", value=st.session_state['textbooks'].get('æ•°å­¦', ''), placeholder="ä¾‹: ãƒãƒ£ãƒ¼ãƒˆå¼")
    tb_eng = st.text_input("è‹±èª", value=st.session_state['textbooks'].get('è‹±èª', ''), placeholder="ä¾‹: æ•™ç§‘æ›¸")
    tb_sci = st.text_input("ç†ç§‘", value=st.session_state['textbooks'].get('ç†ç§‘', ''), placeholder="ä¾‹: è‡ªç”±è‡ªåœ¨")
    tb_soc = st.text_input("ç¤¾ä¼š", value=st.session_state['textbooks'].get('ç¤¾ä¼š', ''), placeholder="ä¾‹: ç”¨èªé›†")
    tb_jpn = st.text_input("å›½èª", value=st.session_state['textbooks'].get('å›½èª', ''), placeholder="ä¾‹: ä¾¿è¦§")
    if st.form_submit_button("ä¿å­˜"):
        st.session_state['textbooks'] = {'æ•°å­¦': tb_math, 'è‹±èª': tb_eng, 'ç†ç§‘': tb_sci, 'ç¤¾ä¼š': tb_soc, 'å›½èª': tb_jpn}
        st.sidebar.success("è¨­å®šä¿å­˜å®Œäº†")

st.sidebar.markdown("---")
if st.sidebar.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’å…¨æ¶ˆå»"):
    st.session_state['data_store'] = {}
    st.session_state['clean_df'] = pd.DataFrame()
    st.session_state['category_map'] = {}
    st.rerun()

# ---------------------------------------------------------
# ğŸ“‚ ãƒ¡ã‚¤ãƒ³ç”»é¢ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & è§£æå®Ÿè¡Œ
# ---------------------------------------------------------
st.markdown("### 1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & è§£æ")
st.caption("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€**ã€ŒAIè§£æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³**ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚æ•´ç†çµæœãŒä¿å­˜ã•ã‚Œã€ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆãŒé«˜é€Ÿã«ãªã‚Šã¾ã™ã€‚")

col_up, col_btn = st.columns([3, 1])

with col_up:
    uploaded_files = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=True, type=['csv'], label_visibility="collapsed")

with col_btn:
    # è§£æå®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆã“ã“ã‚’æŠ¼ã—ãŸæ™‚ã ã‘é‡ã„å‡¦ç†ãŒèµ°ã‚‹ï¼‰
    if st.button("ğŸš€ AIè§£æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
        if uploaded_files:
            # 1. èª­ã¿è¾¼ã¿
            new_count = 0
            for file in uploaded_files:
                df = parse_csv(file)
                if df is not None:
                    st.session_state['data_store'][file.name] = df
                    new_count += 1
            
            if new_count > 0:
                # 2. æ•´ç†å‡¦ç†ï¼ˆã“ã“ã§Gemini 3.0 Proç­‰ãŒå‹•ãï¼‰
                process_and_categorize()
                st.success(f"âœ… {new_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å˜å…ƒã‚’æ•´ç†ã—ã¾ã—ãŸï¼")
            else:
                st.warning("æœ‰åŠ¹ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        elif st.session_state['data_store']:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãªã—ã§ãƒœã‚¿ãƒ³ã ã‘æŠ¼ã—ãŸå ´åˆï¼ˆå†æ•´ç†ï¼‰
            process_and_categorize()
            st.success("âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å†æ•´ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

# ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿è©³ç´°ã‚’è¡¨ç¤º
if not st.session_state['clean_df'].empty:
    df_show = st.session_state['clean_df']
    st.markdown("---")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š å…¨ä½“åˆ†æ", "ğŸ“– å¾©ç¿’ï¼†ãƒ†ã‚¹ãƒˆ", "ğŸ“… åˆæ ¼è¨ˆç”»", "ğŸ“· ç”»åƒæ¡ç‚¹"])

    # --- Tab 1: åˆ†æ ---
    with tab1:
        summary = df_show.groupby(['æ•™ç§‘', 'å†…å®¹'])[['ç‚¹æ•°', 'é…ç‚¹']].sum().reset_index()
        summary['å¾—ç‚¹ç‡(%)'] = (summary['ç‚¹æ•°'] / summary['é…ç‚¹'] * 100).round(1)
        
        # è¡¨ç¤ºç”¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        summary_clean = pd.DataFrame(summary.to_dict('list'))
        summary_clean.columns = [str(c) for c in summary_clean.columns]

        col1, col2 = st.columns([2, 1])
        with col1:
            st.subheader("âš ï¸ å„ªå…ˆå¾©ç¿’å˜å…ƒ")
            st.dataframe(
                summary_clean.sort_values('å¾—ç‚¹ç‡(%)').head(10),
                column_config={"å¾—ç‚¹ç‡(%)": st.column_config.NumberColumn(format="%.1f%%")},
                use_container_width=True,
                hide_index=True
            )
        with col2:
            st.subheader("æ•™ç§‘åˆ¥å¹³å‡")
            sub_sum = df_show.groupby('æ•™ç§‘')[['ç‚¹æ•°', 'é…ç‚¹']].sum().reset_index()
            sub_sum['å¾—ç‚¹ç‡'] = (sub_sum['ç‚¹æ•°']/sub_sum['é…ç‚¹']*100).round(1)
            
            sub_sum_clean = pd.DataFrame(sub_sum.to_dict('list'))
            sub_sum_clean.columns = [str(c) for c in sub_sum_clean.columns]
            st.dataframe(sub_sum_clean, hide_index=True)

    # --- Tab 2: å¾©ç¿’ ---
    with tab2:
        st.subheader("AIå®¶åº­æ•™å¸«ã«ã‚ˆã‚‹æŒ‡å°")
        c1, c2 = st.columns(2)
        with c1: 
            sel_sub = st.selectbox("æ•™ç§‘", summary['æ•™ç§‘'].unique())
        with c2: 
            sel_top = st.selectbox("å˜å…ƒ", summary[summary['æ•™ç§‘']==sel_sub].sort_values('å¾—ç‚¹ç‡(%)')['å†…å®¹'])
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        target_rows = df_show[(df_show['æ•™ç§‘']==sel_sub) & (df_show['å†…å®¹']==sel_top)]
        rate = (target_rows['ç‚¹æ•°'].sum() / target_rows['é…ç‚¹'].sum() * 100).round(1)
        original_topics = target_rows['è©³ç´°'].unique().tolist()
        original_topics_str = "ã€".join([str(t) for t in original_topics])
        
        st.info(f"å˜å…ƒ: **{sel_top}** (å¾—ç‚¹ç‡: {rate}%)")
        st.caption(f"è©³ç´°: {original_topics_str}")
        
        book = st.session_state['textbooks'].get(sel_sub, "å‚è€ƒæ›¸")
        
        if st.button("â‘  å¾©ç¿’ãƒã‚¤ãƒ³ãƒˆã‚’èã"):
            with st.spinner(f"AI({MODEL_NAME})ãŒæ€è€ƒä¸­..."):
                p = f"""
                æ–°æ½Ÿé«˜æ ¡å¿—æœ›ã®ç”Ÿå¾’ã¸ã®æŒ‡å°ã€‚
                æ•™ç§‘: {sel_sub}
                è‹¦æ‰‹ã‚«ãƒ†ã‚´ãƒª: {sel_top}ï¼ˆå…ƒã®å˜å…ƒå: {original_topics_str}ï¼‰
                å¾—ç‚¹ç‡: {rate}%
                ä½¿ç”¨å‚è€ƒæ›¸: {book}
                
                ä¸Šè¨˜ã«åŸºã¥ãã€æ–°æ½Ÿé«˜æ ¡åˆæ ¼ãƒ¬ãƒ™ãƒ«ã«å¼•ãä¸Šã’ã‚‹ãŸã‚ã®å…·ä½“çš„ãªå¾©ç¿’ãƒã‚¤ãƒ³ãƒˆã€ç†è§£ã®æ·±ã•ã®ç›®å®‰ã€ãƒã‚§ãƒƒã‚¯é …ç›®3ã¤ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
                """
                st.session_state['guide'] = ask_gemini_robust(p)
        
        if 'guide' in st.session_state:
            st.markdown(st.session_state['guide'])
            if st.button("â‘¡ ç¢ºèªãƒ†ã‚¹ãƒˆã‚’ä½œæˆ"):
                with st.spinner("å•é¡Œä½œæˆä¸­..."):
                    p2 = f"æ–°æ½Ÿé«˜æ ¡å…¥è©¦ãƒ¬ãƒ™ãƒ«ã€‚{sel_sub}ã®ã€Œ{sel_top}ã€ï¼ˆè©³ç´°: {original_topics_str}ï¼‰ã«é–¢ã™ã‚‹å®Ÿè·µå•é¡Œã‚’1å•ä½œæˆã—ã€è§£ç­”ã¨è§£èª¬ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚"
                    st.session_state['test'] = ask_gemini_robust(p2)
        
        if 'test' in st.session_state:
            st.markdown("---")
            st.markdown(st.session_state['test'])

    # --- Tab 3: è¨ˆç”» ---
    with tab3:
        if st.button("åˆæ ¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ"):
            with st.spinner("è¨ˆç”»ç«‹æ¡ˆä¸­..."):
                prompt = f"ä»Šæ—¥({datetime.date.today()})ã‹ã‚‰å…¥è©¦({EXAM_DATE})ã¾ã§ã®æ–°æ½Ÿé«˜æ ¡åˆæ ¼ã«å‘ã‘ãŸå­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                st.markdown(ask_gemini_robust(prompt))

    # --- Tab 4: ç”»åƒ ---
    with tab4:
        st.subheader("ğŸ“· ç”»åƒæ¡ç‚¹ï¼†æŒ‡å°")
        col_img1, col_img2, col_img3 = st.columns(3)
        with col_img1:
            img_prob = st.file_uploader("â‘  å•é¡Œç”»åƒ", type=['png', 'jpg', 'jpeg'])
        with col_img2:
            img_user = st.file_uploader("â‘¡ è‡ªåˆ†ã®è§£ç­”ç”»åƒ", type=['png', 'jpg', 'jpeg'])
        with col_img3:
            img_ans = st.file_uploader("â‘¢ æ¨¡ç¯„è§£ç­”ç”»åƒ", type=['png', 'jpg', 'jpeg'])
        
        if img_prob and img_user and img_ans:
            if st.button(f"ğŸš€ æ¡ç‚¹å®Ÿè¡Œ ({MODEL_NAME})"):
                with st.spinner("ç”»åƒã‚’åˆ†æä¸­..."):
                    images = [PIL.Image.open(img_prob), PIL.Image.open(img_user), PIL.Image.open(img_ans)]
                    prompt_v = "æ–°æ½Ÿé«˜æ ¡å¿—æœ›ã€‚3æšã®ç”»åƒï¼ˆå•é¡Œã€ç”Ÿå¾’è§£ç­”ã€æ¨¡ç¯„è§£ç­”ï¼‰ã‹ã‚‰ã€å³å¯†ãªæ¡ç‚¹ã€æ·»å‰Šã€å¼±ç‚¹åˆ†æã€é¡é¡Œã®æç¤ºã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
                    st.markdown(ask_gemini_robust(prompt_v, images))
else:
    st.info("ğŸ‘† ä¸Šè¨˜ã‹ã‚‰CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã€ŒAIè§£æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
